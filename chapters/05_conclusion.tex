\chapter{Conclusion}\label{chapter:conclusion}

% NO CITATIONS HERE !!!

% Feature space of VGG16

In this thesis, we approached the problem of unsupervised object extraction from images. In order to address some of the drawbacks associated with current deep learning segmentation models, we proposed utilizing the feature extractor of a pretrained VGG16 model to extract semantic feature descriptors. The intuition is that semantic pixel information can be extracted without relying on pixel-wise annotations for training.

We started by conducting exploratory experiments to check if similar objects had similar representations in the model's feature space. In the first experiment, we extracted the feature images of multiple semantically similar images. Then, we applied PCA to the features and interpreted their projections along the first three principal components as RGB pixel colors. The visualizations showed that similar objects lied close to each other in the feature space of the model. The second experiment aimed at visualizing small-scale feature similarities found in the complete BSD500 dataset. Firstly, the feature representations of all the images from BSD500 were extracted. Then, FSLIC was employed to generate a superpixel segmentation of the feature images. Finally, t-SNE was used to visualize the local similarity of the superpixel centers in the feature space. The t-SNE plot showed that the model has gained a higher-level interpretation of small image details that was primarily based on color patterns and textures.

We then conducted a series of segmentation experiments on two datasets: BSD500 and Cityscapes. The aim of the experiments was to cluster similar pixels found in feature images extracted at different depths of the model, such that each cluster would correspond to a real-world object in the image. The clusters were then shown as different regions in a final segmentation mask.

Concerning the images from BSD500, we found that good results were obtained by extracting the clusters from the deepest block of VGG16's feature extractor. Our first method generated a superpixel segmentation of the feature image using FSLIC. Afterwards, the superpixels were clustered together to generate the final segmentation of the image. A second approach clustered the pixels of the feature image directly using k-means. The generated segmentation masks were able to represent the image contents and distinguish between the different objects similarly to how a human would. Additionally, the regions in the mask had a stronger boundary adherence compared to the regions obtained from the superpixel-based approach. Concerning the images from Cityscapes, the regions in the generated segmentation masks did not represent real-world objects and grouped spatially non-coherent pixels. Future work can consider further processing the feature representations of more complex images before clustering them. 

The segmentation results of BSD500 have shown that a simple model such as VGG16 can be trained for classification and later used for object extraction from images. Furthermore, preliminary work with other simple classification models, such as AlexNet \parencite{krizhevsky2012imagenet}, suggest these models can replace VGG16 in the pipeline.

The final experiment analyzed the feature maps of two classes of images. The goal of the experiment was to uncover the feature map similarities and differences between the objects in the foregrounds of the images. The analysis process relied on computing a similarity score based on the Kolmogorov-Smirnov test. The score was then used to compare the feature map distributions on an intra- and inter-class level. The visualizations of the (dis-)similarly distributed feature maps revealed similarities and differences between the images, such as the presence or absence of a small texture patch or a unique part of the object.

Concerning future work, the segmentation masks could be further processed for refinement purposes. Methods such as superpixel majority voting (see \autoref{chapter:contour_refinement}) can be employed to fix some of the region contours in the mask. Moreover, our findings suggest that future works could be done towards semantic labelling of the clusters. After the clusters are extracted from the feature image, they could be semantically labelled by comparing their centers to a set of known vectors in a lookup table. The semantically labelled masks can then be utilized for automatic decision-making tasks.